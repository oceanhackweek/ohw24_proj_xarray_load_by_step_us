{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b96a8cd-8b38-4372-b76c-10fcdb75b967",
   "metadata": {},
   "source": [
    "# OceanHackWeek2024 - Project Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797bad6d-39f2-497e-ad01-4b4c8b8632e1",
   "metadata": {},
   "source": [
    "## Project - Xarray accessor \"load-by-step\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eb701c-10b6-4ce4-aa29-19d9d2d4498e",
   "metadata": {},
   "source": [
    "### Project description:\n",
    "\n",
    "**A Xarray accessor to download large quantities of data automatically splitting a large request in smaller requests.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8553fc00-e8d3-4c94-bc3b-116054ee7726",
   "metadata": {},
   "source": [
    "Note: A Xarray accessor is simillar to a child class, e.g.:\n",
    "\n",
    "```\n",
    "MyClass(xr.DataArray):\n",
    "    def my_method(self, arg1, arg2):\n",
    "        ...\n",
    "```\n",
    "\n",
    "But it is the recommended way of doing thins with Xarray as show [here](https://docs.xarray.dev/en/stable/internals/extending-xarray.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b24f56-4203-464d-b6ac-b6b14c9be1a9",
   "metadata": {},
   "source": [
    "### Project Members:\n",
    "\n",
    "* Marcelo Andrioni\n",
    "* João Pedro Amorin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8c5db7-b711-41b8-963b-d587b9fa1d94",
   "metadata": {},
   "source": [
    "### Which problem are we trying to solve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f775d75c-5451-4362-9c46-9679e9615cfe",
   "metadata": {},
   "source": [
    "**TL;DR: Downloading large amounts of data from remote servers (e.g.: THREDDS, HYRAX, etc.) using Xarray without server timeouts or silent failures and saving it in a \"reasonable\" (not thousands!!!) number of files for further analysis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e6f8ed-2b8c-4be5-8827-df9175c826ae",
   "metadata": {},
   "source": [
    "The Xarray module is extensively used in the geosciences for its ability to handle multi-dimensional data, provide metadata support, and offer a wide range of analysis and visualization capabilities. Its intuitive interface and compatibility with other Python libraries make it a valuable tool for geoscientific research and data analysis. It's basically Pandas for N-dimensional data.\n",
    "\n",
    "By default, Xarray tries to do everything in \"lazy\" mode until the data has to be acctually loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd7ae5d9-8827-4b3d-863e-a0c45839218e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "\n",
    "ds = xr.tutorial.open_dataset(\"air_temperature_gradient\")\n",
    "da = ds[\"Tair\"]\n",
    "print(da._in_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7840ac-ac13-4332-829e-28a70b50dbd5",
   "metadata": {},
   "source": [
    "If you perform an operation that really needs the data, like multiplication, saving to disk, etc, then everything is actually loaded in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1000d3a-280e-4093-a34b-d74c8b168fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "foo = da * 2\n",
    "print(da._in_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2d82ae-f48a-48bb-a719-e0dddd332d5b",
   "metadata": {},
   "source": [
    "This is not a problem if you are working with local Datasets/DataArrays or small (few MB order of magnitude) remote Datasets/DataArrays.\n",
    "But if you are accessing data from a remote server like [THREDDS](https://www.unidata.ucar.edu/software/tds/), [Hyrax](https://www.opendap.org/software/hyrax-data-server/), [ERDDAP](https://github.com/ERDDAP/erddap), etc, large requests (> 100MB order of magnitude) can fail.\n",
    "\n",
    "These kind of servers are the \"default\" mode of serving model hindcast/forecast and satellite data on the interet. Some examples:\n",
    "\n",
    "* [HYCOM](https://tds.hycom.org/thredds/catalog.html)\n",
    "* [NCEI](https://www.ncei.noaa.gov/thredds/catalog.html)\n",
    "* [NOMADS](https://nomads.ncep.noaa.gov/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cdce97-5b17-4db2-9508-c59dd0c0b62a",
   "metadata": {},
   "source": [
    "Due to the multitude of possible configuration options for these servers, the requests can even fail \"silentlty\", that is, without raising an error.\n",
    "When this happens, the download seems to finish without problems, but in fact the data is full of NaN, zeros, or garbage values like 1e39.\n",
    "\n",
    "Thin can severely impact any subsequent workflows that depend on the data.\n",
    "\n",
    "An example would be trying to download current forecast data to run an oil simulation, getting data filled with zeros, and using that data in the oil drift model without realizing it...\n",
    "\n",
    "It's a wild example, but it actually happened to me. Since the oil drift model also used the wind forecast as a forcing, the oil particles didn't just stay in place. Only later did I realize that the currents were not used in the simulation and everything had to be redone. It was not a real oil spill, but it could have been."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe8b41-77a1-4b1a-8de9-9c7b147e0290",
   "metadata": {},
   "source": [
    "One way of solving this is to split a large request in smaller requests, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe0101f-4f11-4cda-b9da-a40ddccc820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dts = pd.date_range(\"2000-01-01\", \"2024-12-31\", freq=\"1D\")\n",
    "url = \"http://foobar\"\n",
    "\n",
    "for dt in dts[0:-1]:\n",
    "    with xr.open_dataset(url).sel(time=slice(dt, dt - np.timedelta64(1,'s'))):\n",
    "        ds.to_netcdf(f\"foobar_{%Y%m%d:dt}.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfb0a4e-e901-487f-abd1-1d640bbdcbf2",
   "metadata": {},
   "source": [
    "But with this approach, at the end the user will have 9132 files that will have to \"joined\" (with NCO, CDO, xr.open_mfdataset, etc) to perform the desired analysis.\n",
    "It can be done, but this adds a lot of gruntwork and introduces additional points of failure in the whole process. For someone that \"just wants to do an analysis\", having to learn all these different methods and libraries can consume a precious ammount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb185a9-af3a-4a9d-a5d9-5c8749aefacc",
   "metadata": {},
   "source": [
    "### Proposed solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c4cf89-8513-45bf-859f-2f48133b195d",
   "metadata": {},
   "source": [
    "**TL;DR: a Xarray accessor that splitts a large requests in smaller requests internally and returns a single Dataset/DataArray to the user, all in a single line of code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed4ce0-d3cd-425a-9c5f-9cab4d28448b",
   "metadata": {},
   "source": [
    "During OHW24 we developed a python module called \"Xarray load-by-step\". The main code can be seen [here](https://github.com/oceanhackweek/ohw24_proj_xarray_load_by_step_us/blob/main/src/load_by_step/_load_by_step.py).\n",
    "\n",
    "If anybody wants to follow allong, the module can be installed on your \"conda env\" with the following command:\n",
    "\n",
    "`pip install load_by_step@git+https://github.com/oceanhackweek/ohw24_proj_xarray_load_by_step_us`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33620f9c-ea95-4f8e-b05a-254d90580237",
   "metadata": {},
   "source": [
    "You start by importing xarray and the new module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b195757b-7ace-4922-b112-f9883cc8bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import load_by_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5ae500-8a60-4c8d-a353-9ff9ea8080f8",
   "metadata": {},
   "source": [
    "If you check the new module, there is almost nothing there that the user can call directly. Basically just an option in set_option to disable TQDM and don't show the progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "210e87bf-b3ad-4a15-b1bf-764bc55bb5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DALoadByStep',\n",
       " 'DSLoadByStep',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_load_by_step',\n",
       " '_options',\n",
       " 'get_options',\n",
       " 'set_options']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(load_by_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22160d25-61b1-47fe-9582-ec96532b95e1",
   "metadata": {},
   "source": [
    "To acctually use the new module you need to look for the \"lbs\" module in the Dataset/DataArray instance scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f52f5739-9028-4fe5-ad42-e6587d3bfc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<load_by_step._load_by_step.DALoadByStep object at 0x7de831563f40>\n"
     ]
    }
   ],
   "source": [
    "ds = xr.tutorial.open_dataset(\"air_temperature_gradient\")\n",
    "da = ds[\"Tair\"]\n",
    "print(da.lbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e498e5-e938-4baa-86cc-407801487a3d",
   "metadata": {},
   "source": [
    "Now you are able to call the loading methods. For a DataArray the available methods are `load_by_step` and `load_by_bytesize`.\n",
    "\n",
    "**Note**: For this first demonstration we are going to use a small local file. Later on, we are going to shown some \"real life\" examples downloading data from remote servers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54193f1c-3c89-4a05-8076-5bf9d86099ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading '13.2KB' of 'Tair' between time=[2014-12-30T18:00:00.000000000, 2014-12-31T18:00:00.000000000]: 100%|█████████████████████████████| 584/584 [00:03<00:00, 167.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False True\n"
     ]
    }
   ],
   "source": [
    "da2 = da.lbs.load_by_step(time=5)\n",
    "print(da._in_memory, da2._in_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76c1126-a089-4d5d-ab1a-8e042019c32e",
   "metadata": {},
   "source": [
    "The command above is going to split the loading process of the DataArray in blocks of 5 elements along the time dimension. The progress bar shows the size (in KB, MB, etc) of the block and the start and ending elements.\n",
    "\n",
    "You can even split along two or more dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c55741c7-f734-4dba-8cca-8f7b10cb75d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading '23.0KB' of 'Tair' between time=[2014-12-27T00:00:00.000000000, 2014-12-31T18:00:00.000000000], lon=[275.0, 330.0]: 100%|███████████| 60/60 [00:00<00:00, 123.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False True\n"
     ]
    }
   ],
   "source": [
    "da2 = da.lbs.load_by_step(time=100, lon=30)\n",
    "print(da._in_memory, da2._in_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea838426-eb1b-40a0-a15f-ed938164c654",
   "metadata": {},
   "source": [
    "You can see some more arguments in the docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b659ea70-cf59-4749-a288-e68d1abc3039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_by_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mindexers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAnnotated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mseconds_between_requests\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAnnotated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mindexers_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAnnotated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mxarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Load the DataArray in memory splitting the loading process along one\n",
       "or more dimensions.\n",
       "\n",
       "This is useful to download large quantities of data from a THREDDS\n",
       "server automatically breaking the large request in smaller requests\n",
       "to avoid server timeout.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "indexers : dict, optional\n",
       "    A dict with keys matching dimensions and values given by positive\n",
       "    integers.\n",
       "    One of indexers or indexers_kwargs must be provided.\n",
       "seconds_between_requests : float, optional\n",
       "    Wait time in seconds between requests.\n",
       "**indexers_kwargs : {dim: indexer, ...}, optional\n",
       "    The keyword arguments form of ``indexers``.\n",
       "    One of indexers or indexers_kwargs must be provided.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "xr.DataArray\n",
       "    DatArray loaded in memory.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "This example reads data from a local file just for demonstration\n",
       "purpose. A real aplication would be to read data from a THREDDS server.\n",
       "\n",
       ">>> ds = xr.tutorial.open_dataset(\"air_temperature_gradient\")\n",
       ">>> da = ds[\"Tair\"]\n",
       ">>> da._in_memory\n",
       "False\n",
       ">>> da2 = da.lbs.load_by_step(time=500, lon=30, seconds_between_requests=1)\n",
       ">>> da2._in_memory\n",
       "True\n",
       "\u001b[0;31mFile:\u001b[0m      ~/ohw24_proj_xarray_load_by_step_us/load_by_step/_load_by_step.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "da.lbs.load_by_step?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc4099-b780-4af4-b59d-9a695af1a04d",
   "metadata": {},
   "source": [
    "For DataArrays, you can also load by bytesize instead of worrying about how many \"steps\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2fa21ff4-ff5a-4fc1-ab37-2f1d9741d45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading '90.1KB' of 'Tair' between time=[2014-12-23T12:00:00.000000000, 2014-12-31T18:00:00.000000000]: 100%|███████████████████████████████| 79/79 [00:00<00:00, 127.95it/s]\n"
     ]
    }
   ],
   "source": [
    "da2 = da.lbs.load_by_bytesize(time=\"100KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22885459-399a-4f31-adc5-4a2372bee040",
   "metadata": {},
   "source": [
    "Unlike `load_by_step`, at the moment `load_by_bytesize` only allows splitting along one dimension. This can be a problem, for example, if a single time step of your dataset is larger than the bytesize limit. This can happen when getting data from high resolution ocean models:\n",
    "3600 (lons) x 1800 (lats) x 50 (depths) x 4 (bytes) = 1.2 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b401c3e0-7540-4c5e-ad30-5ec61d5dc308",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "It is not possible to load blocks of '1.0KB' along dimension 'time' even when using step=1. Consider increasing the size or calling split_by_step and splitting along a second dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m da2 \u001b[38;5;241m=\u001b[39m \u001b[43mda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_by_bytesize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1KB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/work/lib/python3.10/site-packages/pydantic/validate_call_decorator.py:60\u001b[0m, in \u001b[0;36mvalidate_call.<locals>.validate.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(function)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_function\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalidate_call_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/work/lib/python3.10/site-packages/pydantic/_internal/_validate_call.py:96\u001b[0m, in \u001b[0;36mValidateCallWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 96\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpydantic_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mArgsKwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__return_pydantic_validator__:\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__return_pydantic_validator__(res)\n",
      "File \u001b[0;32m~/ohw24_proj_xarray_load_by_step_us/load_by_step/_load_by_step.py:321\u001b[0m, in \u001b[0;36mDALoadByStep.load_by_bytesize\u001b[0;34m(self, indexers, seconds_between_requests, **indexers_kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mda\u001b[38;5;241m.\u001b[39mlbs\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 321\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is not possible to load blocks of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbytesize_to_human_readable(bytesize)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m along dimension\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m even when using step=1. Consider increasing the size\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or calling split_by_step and splitting along a second\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_by_step(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{dim: step},\n\u001b[1;32m    329\u001b[0m                          seconds_between_requests\u001b[38;5;241m=\u001b[39mseconds_between_requests)\n",
      "\u001b[0;31mValueError\u001b[0m: It is not possible to load blocks of '1.0KB' along dimension 'time' even when using step=1. Consider increasing the size or calling split_by_step and splitting along a second dimension."
     ]
    }
   ],
   "source": [
    "da2 = da.lbs.load_by_bytesize(time=\"1KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b84612-f796-4717-b517-04b3b1f81dd6",
   "metadata": {},
   "source": [
    "We are currenlty working on a new version that can split the request along multiple dimensions following a prefered order of dimensions, e.g.:\n",
    "\n",
    "`da.lbs.load_by_bytesize(bytesize=\"50MB\", dims=[\"time\", \"lat\", \"lon\"])`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f6aef6-ba81-4247-a605-2be2bb1dd092",
   "metadata": {},
   "source": [
    "Besides the DataArray accessor, we also have a Dataset accessor. This will automatically iterate over all the DataArrays in the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "13e1fe4d-3816-4bd5-a84a-6cf632c786d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tair False\n",
      "dTdx False\n",
      "dTdy False\n"
     ]
    }
   ],
   "source": [
    "for var in list(ds.data_vars):\n",
    "    print(var, ds[var]._in_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca2e78e5-fe77-4d38-ab4d-85fce10ee1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading '53.0KB' of 'Tair' between time=[2014-12-27T00:00:00.000000000, 2014-12-31T18:00:00.000000000]: 100%|████████████████████████████████| 30/30 [00:00<00:00, 99.36it/s]\n",
      "Loading '106.0KB' of 'dTdx' between time=[2014-12-27T00:00:00.000000000, 2014-12-31T18:00:00.000000000]: 100%|██████████████████████████████| 30/30 [00:00<00:00, 113.79it/s]\n",
      "Loading '106.0KB' of 'dTdy' between time=[2014-12-27T00:00:00.000000000, 2014-12-31T18:00:00.000000000]: 100%|██████████████████████████████| 30/30 [00:00<00:00, 157.09it/s]\n"
     ]
    }
   ],
   "source": [
    "ds2 = ds.lbs.load_by_step(time=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e081285b-7a37-4320-9dfb-1531509df25e",
   "metadata": {},
   "source": [
    "With this, now we have the full Dataset in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "471c124b-bcbb-4fe0-ad64-fdaac6e42b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tair False True\n",
      "dTdx False True\n",
      "dTdy False True\n"
     ]
    }
   ],
   "source": [
    "for var in list(ds.data_vars):\n",
    "    print(var, ds[var]._in_memory, ds2[var]._in_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1059f354-8375-4a6d-b688-e1c5439328b2",
   "metadata": {},
   "source": [
    "This module attempts to solver the server access bottleneck problem. But at some point, the ammount of physical RAM memory available on the local computer will become a new problem.\n",
    "\n",
    "As an example: most computers nowadays have at least 8 or 16GB of RAM. Assuming that the Operational System and other programs use an average of 4GB (assume way less for Linux and way more for Windows), this leaves 12GB of available memory. For a large Dataset with a lot of variables (DataArrays), even 12GB can not be enough. In these case we recommend the use of the `load_and_save_by_step` method.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6669b8eb-f7fa-4dc3-92bc-4d05a094c190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading '53.0KB' of 'Tair' between time=[2014-12-27T00:00:00.000000000, 2014-12-31T18:00:00.000000000]: 100%|███████████████████████████████| 30/30 [00:00<00:00, 100.59it/s]\n",
      "/home/eani/ohw24_proj_xarray_load_by_step_us/load_by_step/_load_by_step.py:496: SerializationWarning: saving variable Tair with floating point data as an integer dtype without any _FillValue to use for NaNs\n",
      "  da_in_memory.to_netcdf(outfile,\n",
      "Loading '106.0KB' of 'dTdx' between time=[2014-12-27T00:00:00.000000000, 2014-12-31T18:00:00.000000000]: 100%|██████████████████████████████| 30/30 [00:00<00:00, 196.88it/s]\n",
      "Loading '106.0KB' of 'dTdy' between time=[2014-12-27T00:00:00.000000000, 2014-12-31T18:00:00.000000000]: 100%|██████████████████████████████| 30/30 [00:00<00:00, 126.81it/s]\n"
     ]
    }
   ],
   "source": [
    "ds.lbs.load_and_save_by_step(time=100, outfile=\"/tmp/foobar.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8586d831-56cf-4a2d-9efb-fc0b678fe5e0",
   "metadata": {},
   "source": [
    "This method loads each DataArray into memory, saves it to disk, and releases memory to load the next DataArray. With this the physical RAM memory available on the local computer only has to be capable of holding a single DataArray at a time, instead of the whole Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98afdbee-2972-4382-8450-0babc5ac4eec",
   "metadata": {},
   "source": [
    "The previous examples made use of small local files for demostration purposes. Now we are going to show some real life examples accessing data from remote servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd0e4aa-2880-4f50-b484-b83270f3371c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
